from kafka import KafkaProducer
from kafka.admin import KafkaAdminClient, NewTopic
from kafka.errors import KafkaError, NoBrokersAvailable
import json
import random
import time
import datetime
import logging
import signal
import sys
import socket
import threading
import os

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(os.path.join(os.path.dirname(
            os.path.abspath(__file__)), "producer.log"))
    ]
)
logger = logging.getLogger("EnergyProducer")

# Global variables
running = True
kafka_available = False
clients = []
stats = None


def load_statistics():
    """Load statistics from the file generated by the ingest.py"""
    stats_path = "/app/data/energy_statistics.json"
    default_stats = {
        "hourly_stats": [{"hour_of_day": h, "avg_active_power": 2.0, "stddev_active_power": 0.5,
                         "min_active_power": 0.5, "max_active_power": 5.0,
                          "avg_voltage": 230, "stddev_voltage": 5.0,
                          "min_voltage": 220, "max_voltage": 240} for h in range(24)],
        "weekday_stats": [{"hour_of_day": h, "weekday_avg_power": 2.0} for h in range(24)],
        "weekend_stats": [{"hour_of_day": h, "weekend_avg_power": 2.5} for h in range(24)],
        "winter_stats": {"winter_avg_power": 2.5, "winter_stddev_power": 0.6},
        "summer_stats": {"summer_avg_power": 2.3, "summer_stddev_power": 0.5},
        "other_season_stats": {"other_season_avg_power": 2.0, "other_season_stddev_power": 0.4},
        "overall_stats": {"overall_avg_power": 2.2, "overall_stddev_power": 0.5,
                          "overall_min_power": 0.2, "overall_max_power": 6.0}
    }

    try:
        if os.path.exists(stats_path):
            with open(stats_path, 'r') as f:
                loaded_stats = json.load(f)
                logger.info(f"Loaded statistics from {stats_path}")
                # Validate loaded stats
                if all(key in loaded_stats for key in default_stats.keys()):
                    return loaded_stats
                else:
                    logger.warning(
                        "Statistics file has missing keys, using default values")
                    return default_stats
        else:
            logger.warning(
                f"Statistics file not found at {stats_path}, using default values")
            return default_stats
    except Exception as e:
        logger.error(f"Error loading statistics: {e}")
        return default_stats


def connect_producer(max_retries=5):
    """Connect to Kafka with retry logic"""
    global kafka_available
    for attempt in range(max_retries):
        try:
            producer = KafkaProducer(
                bootstrap_servers='kafka:9092',
                value_serializer=lambda v: json.dumps(v).encode('utf-8'),
                acks='all',
                retries=3,
                retry_backoff_ms=500,
                request_timeout_ms=5000
            )
            logger.info("Connected to Kafka successfully")
            kafka_available = True
            return producer
        except NoBrokersAvailable:
            logger.warning(
                f"No Kafka brokers available (attempt {attempt+1}/{max_retries})")
            time.sleep(min(2 ** attempt, 10))
        except KafkaError as e:
            logger.warning(
                f"Kafka connection failed: {e} (attempt {attempt+1}/{max_retries})")
            time.sleep(min(2 ** attempt, 10))
    logger.error("Failed to connect to Kafka after maximum retries")
    kafka_available = False
    return None


def create_topic(max_retries=3):
    """Create Kafka topic if it doesn't exist"""
    for attempt in range(max_retries):
        try:
            admin_client = KafkaAdminClient(
                bootstrap_servers='kafka:9092',
                client_id='energy-admin',
            )
            topics = admin_client.list_topics()
            if "energy-stream" in topics:
                logger.info("Topic 'energy-stream' already exists")
                admin_client.close()
                return True
            topic_list = [NewTopic(
                name="energy-stream",
                num_partitions=1,
                replication_factor=1,
                topic_configs={"retention.ms": "86400000"}
            )]
            admin_client.create_topics(
                new_topics=topic_list, validate_only=False)
            logger.info("Created Kafka topic: energy-stream")
            admin_client.close()
            return True
        except Exception as e:
            logger.warning(
                f"Failed to create topic (attempt {attempt+1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2)
    logger.error("Failed to create Kafka topic after maximum retries")
    return False


def start_socket_server(port=9999):
    """Start socket server to send data to clients"""
    global clients
    try:
        socket_server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        socket_server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        socket_server.bind(('0.0.0.0', port))
        socket_server.listen(5)
        logger.info(f"Socket server started on port {port}")

        def accept_connections():
            global running, clients
            socket_server.settimeout(1.0)
            while running:
                try:
                    client, addr = socket_server.accept()
                    logger.info(f"New client connected: {addr}")
                    clients.append(client)
                except socket.timeout:
                    clients = [c for c in clients if c.fileno() != -1]
                except Exception as e:
                    if running:
                        logger.error(f"Socket server error: {e}")
            for client in clients:
                try:
                    client.close()
                except:
                    pass
            socket_server.close()

        threading.Thread(target=accept_connections, daemon=True).start()
        return True
    except Exception as e:
        logger.error(f"Failed to start socket server: {e}")
        return False


def get_hourly_stats(hour, stats_dict):
    """Get statistics for a specific hour"""
    for entry in stats_dict["hourly_stats"]:
        if entry["hour_of_day"] == hour:
            return entry
    # Return default values if not found
    return {
        "avg_active_power": 2.0,
        "stddev_active_power": 0.5,
        "min_active_power": 0.5,
        "max_active_power": 5.0,
        "avg_voltage": 230,
        "stddev_voltage": 5
    }


def get_weekday_weekend_factor(hour, day_of_week, stats_dict):
    """Get weekend vs weekday factor based on statistics"""
    is_weekend = day_of_week >= 5  # 5=Sat, 6=Sun

    # Get hourly stats
    weekday_power = None
    weekend_power = None

    for entry in stats_dict["weekday_stats"]:
        if entry["hour_of_day"] == hour:
            weekday_power = entry.get("weekday_avg_power")
            break

    for entry in stats_dict["weekend_stats"]:
        if entry["hour_of_day"] == hour:
            weekend_power = entry.get("weekend_avg_power")
            break

    # If we have both stats, calculate the ratio
    if weekend_power and weekday_power and weekday_power > 0:
        weekend_factor = weekend_power / weekday_power
    else:
        # Default: weekends are 1.3x weekdays
        weekend_factor = 1.3

    return weekend_factor if is_weekend else 1.0


def get_seasonal_factor(month, stats_dict):
    """Get seasonal factor based on statistics"""
    overall_avg = stats_dict["overall_stats"].get("overall_avg_power", 2.0)

    winter_avg = stats_dict["winter_stats"].get("winter_avg_power")
    summer_avg = stats_dict["summer_stats"].get("summer_avg_power")
    other_avg = stats_dict["other_season_stats"].get("other_season_avg_power")

    # Calculate seasonal factors if we have the data
    if winter_avg and summer_avg and other_avg and overall_avg > 0:
        winter_factor = winter_avg / overall_avg
        summer_factor = summer_avg / overall_avg
        other_factor = other_avg / overall_avg
    else:
        # Default seasonal factors
        winter_factor = 1.3
        summer_factor = 1.2
        other_factor = 1.0

    # Apply seasonal factor based on current month
    if month in [12, 1, 2]:  # Winter
        return winter_factor
    elif month in [6, 7, 8]:  # Summer
        return summer_factor
    else:  # Spring/Fall
        return other_factor


def generate_realistic_energy_data():
    """Generate realistic IoT energy data calibrated with historical statistics"""
    global stats

    current_time = int(time.time())
    dt = datetime.datetime.fromtimestamp(current_time)
    hour_of_day = dt.hour
    day_of_week = dt.weekday()  # 0=Mon, 6=Sun
    month = dt.month

    # Get statistical parameters for this hour
    hourly_stats = get_hourly_stats(hour_of_day, stats)

    # Get base power from historical average
    base_power = hourly_stats.get("avg_active_power", 2.0)
    power_stddev = hourly_stats.get("stddev_active_power", 0.5)

    # Apply weekday/weekend factor
    weekday_weekend_factor = get_weekday_weekend_factor(
        hour_of_day, day_of_week, stats)
    base_power *= weekday_weekend_factor

    # Apply seasonal factor
    seasonal_factor = get_seasonal_factor(month, stats)
    base_power *= seasonal_factor

    # Add device-specific variation
    device_id = f"meter_{random.randint(1, 100)}"
    device_num = int(device_id.split('_')[1])

    # Large consumers, average consumers, and small consumers
    device_factor = random.uniform(1.1, 1.3) if device_num < 20 else random.uniform(
        0.7, 0.9) if device_num > 80 else random.uniform(0.9, 1.1)
    base_power *= device_factor

    # Determine if this will be an anomaly (5% chance)
    is_anomaly = random.random() < 0.05

    # For anomalies, generate values outside the normal range
    if is_anomaly:
        # Calculate how many standard deviations from mean for a proper anomaly
        std_devs = random.choice(
            [random.uniform(3.5, 6.0), random.uniform(-5.0, -3.5)])
        active_power = base_power + (std_devs * power_stddev)
        active_power = max(0.01, active_power)  # Ensure power is positive
    else:
        # For normal data, use random variation within expected range
        active_power = random.normalvariate(base_power, power_stddev * 0.7)
        active_power = max(0.01, active_power)  # Ensure power is positive

    # Round to 3 decimal places
    active_power = round(active_power, 3)

    # Generate voltage based on historical values
    mean_voltage = hourly_stats.get("avg_voltage", 230)
    stddev_voltage = hourly_stats.get("stddev_voltage", 5.0)

    if is_anomaly and random.random() < 0.5:  # 50% of anomalies affect voltage
        # Generate anomalous voltage
        voltage_deviation = random.choice(
            [random.uniform(3.5, 5.0), random.uniform(-5.0, -3.5)])
        voltage = int(mean_voltage + (voltage_deviation * stddev_voltage))
        voltage = max(180, min(voltage, 260))  # Keep within physical limits
    else:
        # Generate normal voltage
        voltage = int(random.normalvariate(mean_voltage, stddev_voltage * 0.7))
        voltage = max(215, min(voltage, 245))  # Keep within normal limits

    # Calculate related values
    current = round(active_power / voltage if voltage > 0 else 0, 3)
    reactive_power = round(random.uniform(0.1, 0.3) * active_power, 3)

    # Calculate submetering values that add up to active power
    sub1 = random.uniform(0, active_power * 0.3)
    sub2 = random.uniform(0, (active_power - sub1) * 0.5)
    sub3 = random.uniform(0, max(0, active_power - sub1 - sub2))

    return {
        "timestamp": current_time,
        "device_id": device_id,
        "active_power": active_power,
        "reactive_power": reactive_power,
        "voltage": voltage,
        "current": current,
        "Sub_metering_1": round(sub1, 3),
        "Sub_metering_2": round(sub2, 3),
        "Sub_metering_3": round(sub3, 3),
        "is_injected_anomaly": is_anomaly
    }


def generate_data():
    """Generate and send IoT energy data"""
    global running, kafka_available, clients, stats

    # Load statistics from file
    stats = load_statistics()

    producer = connect_producer()
    if producer:
        create_topic()

    socket_available = start_socket_server()

    if not kafka_available and not socket_available:
        logger.error("No data transport methods available. Exiting.")
        return

    def signal_handler(sig, frame):
        global running
        logger.info("Stopping producer...")
        running = False
        if producer:
            producer.close()
        for client in clients:
            try:
                client.close()
            except:
                pass
        sys.exit(0)

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    logger.info(
        "Starting IoT energy data simulation with calibrated parameters...")
    messages_sent = 0
    errors_count = 0
    start_time = time.time()

    try:
        while running:
            data = generate_realistic_energy_data()
            success = False

            if kafka_available and producer:
                try:
                    producer.send('energy-stream', data)
                    success = True
                except Exception as e:
                    logger.warning(f"Failed to send to Kafka: {e}")
                    errors_count += 1
                    if errors_count > 10:
                        logger.info("Attempting to reconnect to Kafka...")
                        try:
                            producer.close()
                        except:
                            pass
                        producer = connect_producer()
                        errors_count = 0

            # If Kafka sending failed or isn't available, try sending to socket clients
            if not success and clients:
                data_str = json.dumps(data) + '\n'
                disconnected_clients = []
                for client in clients:
                    try:
                        client.sendall(data_str.encode('utf-8'))
                    except Exception as e:
                        logger.debug(f"Failed to send to client: {e}")
                        try:
                            client.close()
                        except:
                            pass
                        disconnected_clients.append(client)

                # Remove disconnected clients
                for dc in disconnected_clients:
                    if dc in clients:
                        clients.remove(dc)

            # Count a message as sent if it was sent successfully to Kafka or any client
            if success or (not success and clients):
                messages_sent += 1
                if messages_sent % 100 == 0:
                    elapsed = time.time() - start_time
                    rate = messages_sent / elapsed
                    logger.info(
                        f"Sent {messages_sent} messages ({rate:.2f} msgs/sec)")
                if random.random() < 0.01:  # Log approximately 1% of messages as samples
                    logger.info(
                        f"Sent {'ANOMALY' if data['is_injected_anomaly'] else 'normal'} data: {data}")

            # Small sleep to control flow rate and CPU usage
            time.sleep(0.1)

    except KeyboardInterrupt:
        logger.info("Producer interrupted by user")
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        import traceback
        logger.error(traceback.format_exc())
    finally:
        running = False
        if producer:
            try:
                producer.flush(timeout=10)  # Flush any pending messages
                producer.close(timeout=5)   # Close the producer gracefully
                logger.info("Kafka producer closed")
            except Exception as e:
                logger.error(f"Error closing Kafka producer: {e}")

        # Close all client connections
        for client in clients:
            try:
                client.close()
                logger.debug("Closed client connection")
            except Exception as e:
                logger.debug(f"Error closing client: {e}")

        logger.info(f"Producer stopped after sending {messages_sent} messages")


if __name__ == "__main__":
    generate_data()
